{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA Workbench imports\n",
    "import ema_workbench\n",
    "from ema_workbench import (\n",
    "    Model, RealParameter, ScalarOutcome, MultiprocessingEvaluator,\n",
    "    ema_logging, Constant, Scenario, HypervolumeMetric,\n",
    "    GenerationalDistanceMetric, EpsilonIndicatorMetric,\n",
    "    InvertedGenerationalDistanceMetric, SpacingMetric, Constraint)\n",
    "from ema_workbench.em_framework.optimization import (\n",
    "    EpsilonProgress, to_problem, ArchiveLogger, epsilon_nondominated)\n",
    "from ema_workbench.analysis import parcoords\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_optimization_outputs(result, convergence, scenario_name, seed, output_dir=\"./archives\"):\n",
    "    \"\"\"\n",
    "    Save the optimization result and convergence data to CSV files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    result_path = os.path.join(output_dir, f\"result__scen{scenario_name}__seed{seed}.csv\")\n",
    "    convergence_path = os.path.join(output_dir, f\"convergence__scen{scenario_name}__seed{seed}.csv\")\n",
    "\n",
    "    result.to_csv(result_path, index=False)\n",
    "    pd.DataFrame(convergence).to_csv(convergence_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(epsilon, nfe, scenarios, model):\n",
    "    \"\"\"\n",
    "    Run optimization using the EMA Workbench.\n",
    "\n",
    "    Parameters:\n",
    "    epsilon (list): The epsilon values for the optimization.\n",
    "    nfe (int): The number of function evaluations.\n",
    "    scenarios (list): List of scenarios to be evaluated.\n",
    "    model: The model to be used for optimization.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing results and convergences.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store results and convergence metrics\n",
    "    results = []\n",
    "    convergences = []\n",
    "    \n",
    "\n",
    "    # Use MultiprocessingEvaluator for parallel processing\n",
    "    with MultiprocessingEvaluator(model) as evaluator:\n",
    "        # Iterate over each scenario\n",
    "        for scenario in scenarios:\n",
    "            # Perform optimization three times for each scenario\n",
    "            for i in range(3):\n",
    "                # Define convergence metrics\n",
    "                convergence_metrics = [\n",
    "                    ArchiveLogger(\n",
    "                        \"./archives\",\n",
    "                        [l.name for l in model.levers],\n",
    "                        [o.name for o in model.outcomes],\n",
    "                        base_filename=\"optimization.tar.gz\",\n",
    "                    ),\n",
    "                    EpsilonProgress(),\n",
    "                ]\n",
    "\n",
    "                # Run the optimization\n",
    "                result, convergence = evaluator.optimize(\n",
    "                    nfe=nfe,\n",
    "                    searchover=\"levers\",\n",
    "                    epsilons=epsilon,\n",
    "                    constraints=None,\n",
    "                    convergence=convergence_metrics,\n",
    "                    reference=scenario,\n",
    "                )\n",
    "\n",
    "                # Create result directory if it does not exist\n",
    "                result_dir = \"./archives\"\n",
    "                os.makedirs(result_dir, exist_ok=True)\n",
    "                \n",
    "                # Save the results and convergence metrics to CSV files\n",
    "                result.to_csv(os.path.join(result_dir, f\"result__scen{scenario.name}__seed{i}.csv\"))\n",
    "                pd.DataFrame(convergence).to_csv(os.path.join(result_dir, f\"convergence__scen{scenario.name}__seed{i}.csv\"))\n",
    "\n",
    "                # Append the results and convergence metrics to the lists\n",
    "                results.append(result)\n",
    "                convergences.append(convergence)\n",
    "    \n",
    "    # Return the results and convergence metrics\n",
    "    return results, convergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_optimization_results(path):\n",
    "    \"\"\"\n",
    "    Collect and combine all optimization result CSVs from a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): Path to the folder containing result CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single DataFrame with all the loaded results.\n",
    "    \"\"\"\n",
    "    result_frames = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".csv\") and filename.startswith(\"result__\"):\n",
    "            full_path = os.path.join(path, filename)\n",
    "            data = pd.read_csv(full_path)\n",
    "            result_frames.append(data)\n",
    "\n",
    "    if not result_frames:\n",
    "        raise ValueError(f\"No result files found in {path}\")\n",
    "\n",
    "    return pd.concat(result_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the archive directory exists\n",
    "os.makedirs(\"archives\", exist_ok=True)\n",
    "\n",
    "# Ensure the 'tmp' subdirectory does not conflict\n",
    "tmp_path = os.path.join(\"archives\", \"tmp\")\n",
    "if os.path.exists(tmp_path):\n",
    "    if os.path.isfile(tmp_path):\n",
    "        # If 'tmp' exists as a file, remove it\n",
    "        os.remove(tmp_path)\n",
    "    elif os.path.isdir(tmp_path):\n",
    "        # If 'tmp' exists as a directory, clear its contents\n",
    "        shutil.rmtree(tmp_path)\n",
    "# Create the 'tmp' directory\n",
    "os.makedirs(tmp_path, exist_ok=True)\n",
    "\n",
    "def load_scenarios_from_csv(filepath):\n",
    "    \"\"\"\n",
    "    Load and convert a scenario CSV file into a list of EMA Workbench Scenario objects.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    scenario_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        scenario_data = {\n",
    "            col: row[col]\n",
    "            for col in df.columns\n",
    "            if col.lower() != \"scenario\" and col.lower() != \"scenario\"\n",
    "        }\n",
    "        scenario_name = row[\"scenario\"] \n",
    "        scenario_obj = Scenario(scenario_name, **scenario_data)\n",
    "        scenario_list.append(scenario_obj)\n",
    "\n",
    "    return scenario_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started with 8 workers\n",
      "100%|████████████████████████████████████████| 100/100 [00:26<00:00,  3.83it/s]\n",
      "[MainProcess/INFO] optimization completed, found 12 solutions\n",
      "  0%|                                                  | 0/100 [36:01<?, ?it/s]\n",
      "100%|████████████████████████████████████████| 100/100 [00:24<00:00,  4.12it/s]\n",
      "[MainProcess/INFO] optimization completed, found 15 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:19<00:00,  5.15it/s]\n",
      "[MainProcess/INFO] optimization completed, found 20 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:20<00:00,  4.91it/s]\n",
      "[MainProcess/INFO] optimization completed, found 32 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:20<00:00,  4.92it/s]\n",
      "[MainProcess/INFO] optimization completed, found 19 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:24<00:00,  4.11it/s]\n",
      "[MainProcess/INFO] optimization completed, found 24 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:18<00:00,  5.30it/s]\n",
      "[MainProcess/INFO] optimization completed, found 4 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:19<00:00,  5.24it/s]\n",
      "[MainProcess/INFO] optimization completed, found 4 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:20<00:00,  4.89it/s]\n",
      "[MainProcess/INFO] optimization completed, found 4 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:22<00:00,  4.49it/s]\n",
      "[MainProcess/INFO] optimization completed, found 6 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:19<00:00,  5.22it/s]\n",
      "[MainProcess/INFO] optimization completed, found 7 solutions\n",
      "100%|████████████████████████████████████████| 100/100 [00:18<00:00,  5.43it/s]\n",
      "[MainProcess/INFO] optimization completed, found 5 solutions\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Total Investment Costs'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m combined_results \u001b[38;5;241m=\u001b[39m gather_optimization_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchives\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Generate and save the parallel coordinates plot\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mplot_parallel_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_coordinates_eps_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meps\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m, in \u001b[0;36mplot_parallel_coordinates\u001b[1;34m(results_df, epsilons, output_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPlot a parallel coordinates graph from selected outcome columns in the results.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m selected_outcomes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected Annual Damage\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected Number of Deaths\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Investment Costs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m---> 10\u001b[0m subset \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_outcomes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m limits \u001b[38;5;241m=\u001b[39m parcoords\u001b[38;5;241m.\u001b[39mget_limits(subset)\n\u001b[0;32m     12\u001b[0m axes \u001b[38;5;241m=\u001b[39m parcoords\u001b[38;5;241m.\u001b[39mParallelAxes(limits)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Total Investment Costs'] not in index\""
     ]
    }
   ],
   "source": [
    "def plot_parallel_coordinates(results_df, epsilons, output_path):\n",
    "    \"\"\"\n",
    "    Plot a parallel coordinates graph from selected outcome columns in the results.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    subset = results_df[model.outcomes]\n",
    "    limits = parcoords.get_limits(subset)\n",
    "    axes = parcoords.ParallelAxes(limits)\n",
    "    axes.plot(subset)\n",
    "\n",
    "    plt.title(f'Parallel Coordinates Plot for Epsilon {epsilons}')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Enable logging at INFO level\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    # Load the model setup for a specific problem formulation\n",
    "    model, _ = get_model_for_problem_formulation(2)\n",
    "\n",
    "    # Load scenario definitions\n",
    "    scenario_file = \"./data/selected_scenarios_outcomes.csv\"\n",
    "    scenario_list = load_scenarios_from_csv(scenario_file)\n",
    "\n",
    "    # Optimization parameters\n",
    "    epsilon_sets = [[1e6,1e6,1e6,1e6,1e6]]\n",
    "    function_evals = 100\n",
    "\n",
    "    for eps in epsilon_sets:\n",
    "        # Run the optimization procedure\n",
    "        results  = run_optimization(eps, function_evals, scenario_list, model)\n",
    "\n",
    "        # Aggregate all results for analysis\n",
    "        combined_results = gather_optimization_results(\"archives\")\n",
    "\n",
    "        # Generate and save the parallel coordinates plot\n",
    "        plot_parallel_coordinates(combined_results, eps, f\"parallel_coordinates_eps_{eps}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_policy(results):\n",
    "    # Initialize a list to store the indices of non-dominated policies\n",
    "    non_dominated_indices = []\n",
    "    # Loop over all pairs of policies\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(results)):\n",
    "            # Skip if it's the same policy\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            # Check if policy i is dominated by policy j\n",
    "            if all(results.iloc[j] <= results.iloc[i]) and any(results.iloc[j] < results.iloc[i]):\n",
    "                break\n",
    "        else:\n",
    "            # If we didn't break from the loop, policy i is not dominated by any other policy\n",
    "            non_dominated_indices.append(i)\n",
    "\n",
    "    # Return only the non-dominated policies\n",
    "    return results.iloc[non_dominated_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
